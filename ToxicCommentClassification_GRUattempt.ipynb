{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is heavily inspired by: *https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout-lb-0-048*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#basic config parameters\n",
    "embed_size = 50\n",
    "max_features = 20000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read data and replace missing values\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "train_de = pd.read_csv(\"train_de.csv\") \n",
    "train_fr = pd.read_csv(\"train_fr.csv\")\n",
    "train_es = pd.read_csv(\"train_es.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train = train.append(train_de, ignore_index=True)\n",
    "train = train.append(train_fr, ignore_index=True)\n",
    "train = train.append(train_es, ignore_index=True)\n",
    "\n",
    "train = train.fillna(\"_na_\")\n",
    "test = test.fillna(\"_na_\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes]\n",
    "list_sentences_test = test[\"comment_text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def translation(df, dest, column=\"comment_text\" ):\n",
    "    \"\"\"\n",
    "    functions gets one df and translates \n",
    "    the specified column into the destined language and back\n",
    "    returns extended dataframe\n",
    "    \"\"\"\n",
    "    print(\"Start translation...\")\n",
    "    translator = Translator()\n",
    "    temp = []\n",
    "    for i in range(len(df)): \n",
    "        if i%10000 == 0:\n",
    "            print(\"Translating entry {} of {} entries.\".format(i, len(df)))\n",
    "        intermediate = translator.translate(df[column].iloc[i], src='en', dest=dest)\n",
    "        temp.append(translator.translate(str(intermediate), src=dest, dest='en'))\n",
    "        \n",
    "    return pd.Series(temp)\n",
    "    \n",
    "def build_and_append(df, list_classes, to_append):\n",
    "    \"\"\"\n",
    "    function gets the df to which to append, the list of classes\n",
    "    and the translated Series which shall be appended\n",
    "    \"\"\"\n",
    "    \n",
    "    to_append=pd.DataFrame(to_append)\n",
    "    for cl in list_classes:\n",
    "        to_append[cl] = df[cl].copy()\n",
    "        \n",
    "    df = df.append(to_append, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from googletrans import Translator\n",
    "x = translation(train, 'ja')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Standard Keras preprocessing, each comment is turned into a list of words indexes of equal length (including truncation/padding)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features, filters='\"()+,.;=[\\\\]^`{}\\t\\n',)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(0.5, 0.25, (nb_words, embed_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Simple bidirectional LSTM with two fully connected layers</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(GRU(100, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Bidirectional(GRU(50, return_sequences=True, dropout=0.15, recurrent_dropout=0.15))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(6, activation='sigmoid')(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 574455 samples, validate on 63829 samples\n",
      "Epoch 1/20\n",
      "574250/574455 [============================>.] - ETA: 1s - loss: 0.0850 - acc: 0.9739\n",
      "Epoch 00001: loss improved from inf to 0.08497, saving model to C:\\WM\\ToxicComments\\save.hd5f\n",
      "574455/574455 [==============================] - 3899s 7ms/step - loss: 0.0850 - acc: 0.9739 - val_loss: 0.0551 - val_acc: 0.9802\n",
      "Epoch 2/20\n",
      "574250/574455 [============================>.] - ETA: 1s - loss: 0.0526 - acc: 0.9810\n",
      "Epoch 00002: loss improved from 0.08497 to 0.05264, saving model to C:\\WM\\ToxicComments\\save.hd5f\n",
      "574455/574455 [==============================] - 3852s 7ms/step - loss: 0.0526 - acc: 0.9810 - val_loss: 0.0496 - val_acc: 0.9819\n",
      "Epoch 3/20\n",
      "574250/574455 [============================>.] - ETA: 1s - loss: 0.0485 - acc: 0.9820\n",
      "Epoch 00003: loss improved from 0.05264 to 0.04851, saving model to C:\\WM\\ToxicComments\\save.hd5f\n",
      "574455/574455 [==============================] - 3960s 7ms/step - loss: 0.0485 - acc: 0.9820 - val_loss: 0.0482 - val_acc: 0.9824\n",
      "Epoch 4/20\n",
      "199250/574455 [=========>....................] - ETA: 40:58 - loss: 0.0457 - acc: 0.9827"
     ]
    }
   ],
   "source": [
    "filepath = r\"C:\\WM\\ToxicComments\"\n",
    "save = r\"C:\\WM\\ToxicComments\\save.hd5f\"\n",
    "tb = r\"C:\\WM\\ToxicComments\\tensorboard\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor = 'loss', min_delta=0, patience=1, verbose=10, mode='min'),\n",
    "    ModelCheckpoint(filepath=save, monitor = 'loss', verbose=1, save_best_only=True),\n",
    "    TensorBoard(log_dir=tb, histogram_freq=1)\n",
    "]\n",
    "\n",
    "model.fit(X_t, y, batch_size=250, epochs=20, callbacks=callbacks, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 203s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict([X_te], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"submission_LSTM5.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
